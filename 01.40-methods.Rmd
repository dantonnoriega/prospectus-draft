## Methods {#methods-1 -}

### Set up {-}

Recall the research question of interest---whether the DUFB incentive increases spending on fresh fruits and vegetables (FV) within stores participating in the program. The outcome variable, in this case, is total spending on FV.

I've considered other possible values for the outcome variable. For example, the proportion of dollars spent per transaction or the total ounces of FV purchased. Each added an extra layer of complication. Using proportion of expenditure per given transaction would vary wildly, particularly with small transactions, and would creating two mass points at zero and one (no FV purchased and only FV purchased). Total ounces depends on the variable and quality of the data. I cannot be sure the data received will contain counts or ounces for fresh fruits. Fresh fruits generally do not have UPC values. Dollars spent (expenditures) on FV gets to the heart of the question and is the data guaranteed to be in the data.

Unfortunately, using the total expenditure of FV is complicated by 3 problems.

1. Purchases not linked to customers.
2. Outcome Variable with non-trivial amount of zeros (corner solutions).
3. Consumers maximize across multiple product types.

The first problem is not specific to the outcome variable. The inability to link purchases to individuals means I cannot use Panel Data Methods at the customer level; I will be unable to look at how customers behavior changes over-time and cannot control for unobserved customer heterogeneity. I'm instead limited to modeling repeated cross-sections over-time with store-level fixed effects.

The second and third problems are directly related to the preferred outcome variable. I anticipate a non-trivial amount of zeros because I expect to observed a large fraction of transaction where no FVs are never purchased. Likewise, FV expenditures are often a part of a basket of goods. Just as I anticipate a non-trivial amount of zeros, I also anticipate that FVs are not purchased independently of other goods. When spending their money, consumers optimize expenditure across a large set of options. This optimization is also complicated by the first problem (cannot link) but I will go into more details later.


### Literature Reviews {-}

**Panel Data to Pseudo-Panel Data**

@verbeek_pseudo-panels_2008 summarizes the pioneering work of @deaton_panel_1985 and @moffitt_identification_1993, both of whom contruct "pseudo-panel" data from repeated cross-sections to produce consistent population estimates. The underlying method is to break repeated cross-sections into groups, averaging over observations in each group, to build fixed-effects models that can be consistently estimated using OLS. This is a middle ground between methods for panel data and methods for pooled cross-section. In other words, it is a second-best OLS alternative given I cannot link purchases to invididuals but I can still link purchases to stores.

My fear, however, is that in building a psuedo-panel for consistent OLS estimation, which solves the first problem


There is a vast literature on consumer purchasing behavior aka choice models (see @train_discrete_2009 for an introductory overview). Multiple-discrete choice models, in particular, have become popular given the increased availability of transaction-level (scanner) data [@dube_multiple_2004;@hendel_estimating_1999]. Multiple-discrete choice models, however, are too product-specific. It is not important for me to model which brand and quality of bananas or carrots were purchased. What is important to me is how much was spent on bananas, carrots, and other fruits and vegetable types versus other non-FV types. Remember, my outcome variable is expenditure on *total* FV spending. I'm far more concerned about whether or not consumers are observed buying *any* FV than I am with the exact types. That said, expenditure on non-FV is also important. *Therefore, what is needed is a model framework flexible enough to handle a continuous outcome variable with a non-trivial amount of zeros (corner solutions) and multiple types of goods.*

**Continuous Outcome Variables and Corner Solutions**

Corner solutions for expenditures---a continuous variable---can occur for various reasons. @pudney_modelling_1989 covers three of the most likely mechanisms producing "true" zeros in cross-sectional data. The first is that the data was gathered in too short a period of time for the purchase to occur. This problem is far more common in cross-sectional data of infrequently purchased goods (i.e. durable goods like cars or refrigerators). The second is due to a supply side factor the customer has no control over. For example, there could be a shortage of FVs or none are available. Imagine searching for FV purchases in collection of convenience store data. Many zeros would exists because some convenience stores do not sell FVs. The third zero results from a customer's decision as a function of prices, preferences, and income constraints. A zero is perhaps observed because FVs are too expensive and the customer can get larger quantities of other, equally or more preferred, foods for the same price. 

I expect first mechanism could apply to food purchases under specific circumstances. If data are left disaggregated or the period of observation is shrunk substantially, zeros for FVs will exists due to infrequency of purchase. For example, customers that make frequent trips to the store may not always buy FVs. A cross-section of data from one day may have a non-zero FV value for the customer but zero the next. The third mechanism applies very naturally to the grocery store environment (classical utility theory). The second will require further verification. I do not expect to find out that there were shortages of FVs in the stores observed, but I could be wrong.

@humphreys_dealing_2013 and @carlevaro_multiple_2016 discuss cross sectional models that accommodate corner solutions---Tobit, *Two-Part*, and *Hurdle* models, specifically.^[@wooldridge_econometric_2010 does not appear to differentiate between "Two-Part" models and "Hurdle" models. I will use it following @humphreys_dealing_2013 language, which does.] The classic "corner solution" regression model is the Tobit model. Corner solutions are utility maximizing and no assumptions are made about the decision not to purchase/consume. In contrast to the Tobit model, the decision to participate in consumption is explicitly formulated in the Two-Part and Hurdle models. Participation is the "first hurdle", the amount purchased/consumed is the "second hurdle". 

In the Two-Part model, the decision to buy is estimate separately and sequentially from how much (amount) to buy; Probit is used to estimate the decision-to-buy process and OLS is used to estimate the amount purchased. The Double Hurdle model estimates both these decision simultaneously via maximum likelihood. The "full" Double Hurdle model allows for correlation between the error terms in the decision and amount/consumption equations [@jones_note_1992]. Imposing the assumption that unobservable factors between the decision and amount decisions are uncorrelated reduces the "full" Double Hurdle to the "Cragg" model [@cragg_statistical_1971].

It is unclear to me if Two-Part or the Double Hurdle models that formalize "participation" are necessary when considering FV purchases. This makes more intuitive sense for something like cigarette or alcohol consumption, where zero-expenditure can be the result of abstention or price/income constraints.^[See @garcia_alternative_1996 and @aristei_cohort_2008 for abstention-style hurdles.] There are, after all, consumers who will never consume cigarettes or alcohol, even if free (abstention). But I'm not sure if an abstention mechanism is reasonable when considering FV purchases. Do consumers really opt out (or abstain) form buying FVs altogether?

An infrequency of purchase mechanism for FVs, however, seems more reasonable. @deaton_statistical_1984 introduced this mechanism as an expansion to the Tobit model. This Double Hurdle has been used to model infrequent purchases of butter, pork, and prepared meals [@yen_modeling_1995; @su_microeconometric_1996; @newman_double-hurdle_2003]. I find it reasonable to expect that, for any given daily cross-section of data, some of the zeros observed will be due to infrequent purchase of FVs.

**Multiple Goods**

The limitation of Tobit and other hurdle models is the estimated demand of a single good. Ideally, expenditure on different types of goods better captures the decisions and purchasing behavior of customers within a grocery store. That is, instead of collapsing all expenditure on FVs into one outcome variable, the model would allow for customers to optimize across *multiple* goods of different types, while also allowing corner solutions. Multiple discrete choice models allow the purchase of multiple types of goods, but do not capture the intensive margin (the amount) of the good purchase/consumed.

@dubin_econometric_1984 construct a *discrete-continuous* model were consumers can select from multiple goods/options but that they are mutually exclusive, perfect substitutes. The *discrete* component of the model captures the decision to buy a non-zero amount; the *continuous* part captures the amount purchased/consumed/utility acquired. The mutually exclusive, perfect substitute condition, however, means that one cannot be observed buying/consuming more than one good/option. 

For any observed trip to the store, zero-expenditure in FV implies non-zero expenditure for some non-FV. Ignoring this seems unwise and I plan, at a minimum, to implement a model optimize along two dimensions---FV and non-FV. The best model I've found is the *multiple* discrete-continuous model extreme value (MDCEV) model introduced by @bhat_multiple_2005. MDCEV is an extension of the Kuhn-Tucker based model developed by @wales_estimation_1983. MDCEV allows non-zero consumption across multiple goods. @kim_modeling_2002 solved the intractability of the @wales_estimation_1983 model. @bhat_multiple_2005 simplified both to be more realistically applicable.

In the next subsection, I will formally introduce the different modeling structures I intend to use in my paper. I will first introduce the basic framework had I expected OLS to be sufficient. I then expand from OLS to Tobit and other Hurdle models before expanding into the more complex MDCEV model. The share goal, across all models, is the best possible measurement of the impact the DUFB incentive has on fruit and vegetable expenditures between participating and non-participating stores.


### Overview of Regression Frameworks {-}

I want to measure the difference in SNAP EBT dollars being spent or redeemed on fresh produce. If the incentive is working, then I should see in increase in SNAP EBT dollars spending on fresh produce within stores implementing the DUFB incentive. I assume the DUFB incentive, if effective, will have a store-level effect. That is, if a store's implementation of DUFB affects individual behavior, the effect should be measurable along multiple cross-sections of observed transactions. 

The Difference-in-Differences (DD) regression is a fitting framework and it will be present in each of the models of my main analysis. A DD analysis makes use of as much data a possible---17 *treated* stores and the 15 *control* stores. I will assume the selection problem outline in the [Store Selection Section](#store-selection-1) can be controlled using store-level fixed-effects. Given I cannot link transaction to individuals, I will instead use the "pseudo-panel" approach [@deaton_panel_1985; @verbeek_pseudo-panels_2008].

A second subsection will explore a Regression Discontinuity (RD) design. This is a consequence of the store selection issue outline in the data section. The RD analysis will be performed on the *11* assigned stores and *12* control stores. In place of fixed effects, I will use the score value calculated previously. In theory, this is a selection on observables problem because I know the procedure used to select DUFB stores. The score value---a linear combination of the observable selection criteria---should therefore serve to control the bias introduced by non-random assignment.

### Difference-in-Differences (DD) Pseudo-Panel {-}

Assume it is possible to observed individual $i$ purchasing goods across $t=1,...,T$ days. Let $y_{it}$ be total FV expenditures for individual $i$ from day $t$. The DD regression is

$$
y_{it} = \gamma_i + \lambda_t + \delta D_{it} + \bm{x'_{it} \beta} + \epsilon_{it}
$$

where $\gamma_i$ are individual fixed-effects, $\lambda_t$ capture daily (time) effects, $D_{it}$ indicates individual $i$'s' participation in DUFB on day $t$, and $\bm{x'_{it}}$ is a vector of observable characteristics about individual $i$'s transaction on day $t$. Idiosyncratic errors are represented by $\epsilon_{it}$ and assumed orthogonal to $\bm{x'_{it}}$ (i.e. $E[x'_{it}u_{it}] = 0$).

Of course, I do not observed individuals, only transactions. Also, more often than not, the same individual would likely observed sporadically within in a given month. That is, were this to be panel data gathered daily from the same $N$ individuals, many of those days would have missing data.

Let's instead think of each day of observed data as a single independent cross-section of $K$ transactions generated by an unknown $N \le K$ individuals across $J$ many stores. Aligned sequentially, these form a repeated cross-sections known as a "pseudo-panel". Each transaction also falls naturally into a grouping structure---the store where it occurred---that is time-invariant, determined prior to the data being collected, and unaffected by treatment status.

This fits the set up for a pseudo-panel data structure introduced by @deaton_panel_1985, where he shows consistent estimation of $\beta$ (and $\delta$) is possible via grouping. In this case, one aggregates over all observed transactions to the store $j$ level, creating a pseudo-panel. The model then becomes

$$
\bar{y}_{jt} = \gamma_j + \lambda_t + \delta D_{jt} + \bm{\bar{x}'_{jt}} {\beta} + \bar{\epsilon}_{jt}, \; j = 1,...,J; \; t=1,...,T
$$

where $\bar{y}_{jt}$ is the average of all observed fruit and vegetable expenditures $y_{it}$ in store $j$ in day $t$. Likewise for $\bm{\bar{x}'_{jt}}$. $\lambda_t$ is unaffected and $\sum_{i \in Store_j} D_{it}}$ collapses to $D_{jt}$. @verbeek_pseudo-panels_2008 suggest is reasonable to assume $\gamma_i$ collapses to $\gamma_j$ if the number of individuals (transactions) in each group (store) is large. This is more than reasonable given the hundreds to thousands of transactions observed in each store per day. 

Note that $\gamma_j$ captures all time-invariant store-level characteristics. This includes the assignment group. The daily effect $\gamma_t$ are included given the amount of data captured at the daily level. I anticipate day-of-the-week and week-of-the-month to matter when analyzing purchasing behavior. $\bm{\bar x'_{jt}}$ represents the average characteristics 

---

**!!BELOW HERE BE DRAGONS (SUPER WORK IN PROGRESS)!!**

### Secondary Analysis: Regression Discontinuity (RD) {-}

In the [Store Selection](#store-selection-1) section, I discussed the construction of the score function $\bm{s} = \widehat{P(\mathbf{D} = 1 | \bm{X}, \bm{N})}$. Given $i = 1,...,n$ stores, the score of each store can be determined via observable data, $s_i = \widehat{P(D_i = 1|\bm{x}_i, n_i)}$. These scores, when ordered, produced perfect separation between treatment stores and control stores (see Figure \@ref(fig:score-plot2)).

An RD design requires a *running variable* where, above some value $c$, the probability of being assigned to the treatment group is $1$. Assume I make the score function $\bm{s}$ my running variable such that $D_i = \bm{1}[s_i \ge c]$.

In my case, assignment $D_i$ is determined by $s_i$ by construction. Recall that $s_i$ is a function estimated on observable covariates. These are the same observable covariates the company used to determine assignment for a subset of stores. I used a linear probability model to estimate the score function and the estimated model perfectly predicted assignment. I then ordered stores by their score value and selected the next 12 unassigned stores.

This problem is that I do not actually know $c$. I only know that $c \in (0.50, 0.64)$. The light gray band in Figure \@ref(fig:score-plot2) displays the possible values of $c$. The problem, in essence, is that I do not have---and never will have---enough stores, so I'm lacking density around where the separation occurs. 

```{r score-plot2, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE, fig.height=4, fig.cap = "Store Score vs Double Up Assignment with Uncertainty Band (light gray)"}

g1 + geom_rect(aes(xmin = .51, xmax = .63, ymin = -Inf, ymax = Inf), fill = 'lightgray')

```

I propose to estimate the RD design using various values of $c$. The perpetual gap means any model estimate to the left or right of some $c_0 \in (0.50, 0.64)$  will have to be extrapolated up to $c_0$.

**Set-up**

The outcome of variable for each store will be the *average amount spent on locally grown produce in a SNAP transaction per day*. The timeframe will be August - December (months $8$ - $12$) of 2016, when the DUFB incentive is place. I decided on using days as the unit of observation to increase the sample amount of data for estimating. I expect there to be enough transactions per day for this to be possible.

Let $y_{i}$ represent the outcome variable where $i=1,...,n$ denotes stores. Let $c$ denote the cutoff; $s_i$ the score computed for store $i$; and $D_i$ the assignment variable. Each draw (or row) of data for store $i$ is a vector $(y_i, s_i, D_i)$ corresponding to a single day. Recall that $y_i$ is a point statistics estimated using a single days worth of transaction data for store $i$. All days will be pooled, creating roughly $30\times5=150$ observations (days) per store.


Let $u_i$ be an error term assumed to be $\mathcal{N}(0, \sigma^2)$.

The RD model I propose is as follows

$$
y_{i} = \alpha + \rho D_i + \gamma (s_i - c) + \delta D_i(s_i - c) + u_{i}
$$

My hope is to produce a graph that looks like the following

[GENERATE EXAMPLE GRAPH. At each point $s_i$, there will be about 150 points drawn in vertical like. This would help visualize the distribution of average-dollars-per-day.]
