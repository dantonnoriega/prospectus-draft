## Methods {#methods-1 -}

### Set up {-}

Recall the research question of interest---whether the DUFB incentive increases spending on fresh fruits and vegetables (FFV) within stores participating in the program. The outcome variable, in this case, is total spending on FFV.

I've considered other possible values for the outcome variable. For example, the proportion of dollars spent per transaction or the total ounces of FFV purchased. Each added an extra layer of complication. Using proportion of expenditure per given transaction would vary wildly, particularly with small transactions, and would creating two mass points at zero and one (no FFV purchased and only FFV purchased). Total ounces depends on the variable and quality of the data. I cannot be sure the data received will contain counts or ounces for fresh fruits. Fresh fruits generally do not have UPC values. Dollars spent (expenditures) on FFV gets to the heart of the question and is the data guaranteed to be in the data.

Unfortunately, using the total expenditure of FFV is complicated by 3 problems.

1. Purchases not linked to customers.
2. Non-trivial amount of zeros (corner solutions).
3. Consumers maximize across multiple goods.

The first complication is a general problem and is not specific to the outcome variable. The inability to link purchases to individuals means I cannot use Panel Data Methods at the customer level; I will be unable to look at how customers behavior changes over-time and cannot control for unobserved customer heterogeneity. I'm instead limited to modeling pooled cross sections over-time with store-level fixed effects.

The second and third problems are directly related to the preferred outcome variable. I anticipate a non-trivial amount of zeros because I expect to observed a large fraction of transaction where no FFVs are never purchased. Likewise, FFV expenditures are often a part of a basket of goods. Just as I anticipate a non-trivial amount of zeros, I also anticipate that FFVs are not purchased independently of other goods. When spending their money, consumers optimize expenditure across a large set of options. This optimization is also complicated by the first problem (cannot link) but I will go into more details later.


### Literature Reviews {-}

Corner solutions for expenditures can occur for various reasons. @pudney_modelling_1989 covers three of the most likely mechanisms producing "true" zeros in cross-sectional data. The first is that the data was gathered in too short a period of time for the purchase to occur. This problem is far more common in cross-sectional data of infrequently purchased goods (i.e. durable goods like cars or refrigerators). The second is due to a supply side factor the customer has no control over. For example, there could be a shortage of FFVs or none are available. Imagine searching for FFV purchases in collection of convenience store data. Many zeros would exists because some convenience stores do not sell FFVs. The third zero results from a customer's decision as a function of prices, preferences, and income constraints. A zero is perhaps observed because FFVs are too expensive and the customer can get larger quantities of other, equally or more preferred, foods for the same price. 

I expect first mechanism could apply to food purchases under specific circumstances. If data are left disaggregated or the period of observation is shrunk substantially, zeros for FFVs will exists due to infrequency of purchase. For example, customers that make frequent trips to the store may not always buy FFVs. A cross-section of data from one day may have a non-zero FFV value for the customer but zero the next. The third mechanism applies very naturally to the grocery store environment (classical utility theory). The second will require further verification. I do not expect to find out that there were shortages of FFVs in the stores observed, but I could be wrong.

@humphreys_dealing_2013 and @carlevaro_multiple_2016 discuss cross sectional models that accommodate corner solutions---Tobit, *Two-Part*, and *Hurdle* models, specifically.^[@wooldridge_econometrics_2010 does not appear to differentiate between "Two-Part" models and "Hurdle" models. I will use it following @humphreys_dealing_2013 language, which does.] The classic "corner solution" regression model is the Tobit model. Corner solutions are utility maximizing and no assumptions are made about the decision not to purchase/consume. In contrast to the Tobit model, the decision to participate in consumption is explicitly formulated in the Two-Part and Hurdle models. Participation is the "first hurdle", the amount purchased/consumed is the "second hurdle". 

In the Two-Part model, the decision to buy is estimate separately and sequentially from how much (amount) to buy; Probit is used to estimate the decision-to-buy process and OLS is used to estimate the amount purchased. The Double Hurdle model estimates both these decision simultaneously via maximum likelihood. The "full" Double Hurdle model allows for correlation between the error terms in the decision and amount/consumption equations [@jones_note_1992]. Imposing the assumption that unobservable factors between the decision and amount decisions are uncorrelated reduces the "full" Double Hurdle to the "Cragg" model [@cragg_statistical_1971].

It is unclear to me if Two-Part or the Double Hurdle models that formalize "participation" are necessary when considering FFV purchases. This makes more intuitive sense for something like cigarette or alcohol consumption (see @garcia_alternative_1996; @aristei_cohort_2008), where zero-expenditure can be the result of abstention or price/income constraints. There are, after all, consumers who will never consume cigarettes or alcohol, even if free (abstention). But I'm not sure if an abstention mechanism is reasonable when considering FFV purchases. Do consumers really opt out (or abstain) form buying FFVs altogether?

An infrequency of purchase mechanism for FFVs, however, seems more reasonable. @deaton_statistical_1984 introduced this mechanism as an expansion to the Tobit model. The Deaton Double Hurdle been used to model infrequent purchases of butter, pork, and prepared meals [@yen_modeling_1995; @su_microeconometric_1996; @newman_double-hurdle_2003]. I find it reasonable to expect that for any given daily cross-section of data, some of the zeros observed will be due to infrequent purchase of FFVs.

**Multiple Goods**

The real limitation of Tobit and other hurdle models is that the utility maximization problem involves only one good. 

A more complicated approach to deal with zeros would be to extend the discrete-continuous model of @dubin_econometric_1984 to the *multiple* discrete-continuous model of @bhat_multiple_2005 and (2008). @dubin_econometric_1984 assume that multiple options exists but that they are mutually exclusive, perfect substitutes. The decision to buy one good means that one cannot be observed buying the other. @bhat_multiple_2005 weakens this constraint by allowing non-zero consumption across multiple goods.

Another option is to model each store as a random value $Y_j$ that spits out expenditures along a mixed distribution. Specifically, a mix of a discrete distribution and continuous distribution where $P(Y_j = 0) > 0$ (discrete) and $P(Y_j = y) = 0$ for all $y > 0$ (continuous). This drops any economically driven foundations and merely provides a way to compare distributions between non-DUFB and DUFB stores.


### Overview of Proposed Methods {-}


I will perform two main analyses: Difference-in-Differences (DD) and Regression Discontinuity (RD). The DD analysis will make use of as much data a possible---17 *treated* stores and the 15 *control* stores. I will assume the selection problem outline in the [Store Selection Section](#store-selection-1) can be controlled using store-level fixed-effects. That is, I will assume the DUFB treatment is strictly exogenous conditional on any time-invariant (observed and unobserved) store-level characteristics.

This is a consequence of the store selection issue outline in the data section. The main analysis will be performed on the 9 *assigned* stores and 12 *control* stores.^[I'm excluding the 2 pilot store because they are never observed without DUFB.] In attempts to make use of as much data as possible, I will also perform a smaller analysis with the 3 *self-selected* stores matched using CEM.^[Please see the Section and Table \@ref(tab:store-class)) for more about *assigned* and *self-selected* stores.] 

Difference-in-Differences (DD) and Regression Discontinuity (RD) will comprise the main analysis. DD will be the only method for the second smaller analysis. I outline each analysis and its methods in the next section.

The unit of analysis will be the store, not the individual; the data cannot be linked to individuals. I assume the DUFB incentive, if effective, will have a store-level effect. That is, if a store's implementation of DUFB affects individual behavior, the effect should be measurable after aggregating over all observed transactions. My proposed analyses depend on this assumption but I am confident the effect will be measurable.

I propose two outcome variables. The first is the proportion of SNAP EBT dollars being spent or redeemed on fresh produce. If the incentive is working, then I should see in increase in SNAP EBT dollars spending on fresh produce. I'm certain this outcome variable will be available. I'm not so certain about the second outcome variable, the total quantity of fresh produce purchased. This depends on whether weight or quantity is included in the data. This will depend on UPC matching. UPCs will be possible, but matching to UPC databases is not always precise. Should matching be poor, discerning what a product is, its weight, etcetera, will be difficult, making the second outcome variable unreliable.

### Main Analysis: Regression Discontinuity (RD) {-}

In the [Store Selection](#store-selection-1) section, I discussed the construction of the score function $\bm{s} = \widehat{P(\mathbf{D} = 1 | \bm{X}, \bm{N})}$. Given $i = 1,...,n$ stores, the score of each store can be determined via observable data, $s_i = \widehat{P(D_i = 1|\bm{x}_i, n_i)}$. These scores, when ordered, produced perfect separation between treatment stores and control stores (see Figure \@ref(fig:score-plot)).

An RD design requires a *running variable* where, above some value $c$, the probability of being assigned to the treatment group is $1$. Assume I make the score function $\bm{s}$ my running variable such that $D_i = \bm{1}[s_i \ge c]$.

In my case, assignment $D_i$ is determined by $s_i$ by construction. Recall that $s_i$ is a function estimated on observable covariates. These are the same observable covariates the company used to determine assignment for a subset of stores. I used a linear probability model to estimate the score function and the estimated model perfecty predicted assignment. I then ordered stores by their score value and selected the next 12 unassigned stores.

This problem is that I do not actually know $c$. I only know that $c \in (0.50, 0.64)$. The light gray band in Figure \@ref(fig:score-plot2) displays the possible values of $c$. The problem, in essence, is that I do not have---and never will have---enough stores, so I'm lacking density around where the separation occurs. 

```{r score-plot2, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE, fig.height=4, fig.cap = "Store Score vs Double Up Assignment with Uncertainty Band (light gray)"}

g1 + geom_rect(aes(xmin = .51, xmax = .63, ymin = -Inf, ymax = Inf), fill = 'lightgray')

```

I propose to estimate the RD design using various values of $c$. The perpetual gap means any model estimate to the left or right of some $c_0 \in (0.50, 0.64)$  will have to be extrapolated up to $c_0$.

**Set-up**

The outcome of variable for each store will be the *average amount spent on locally grown produce in a SNAP transaction per day*. The timeframe will be August - December (months $8$ - $12$) of 2016, when the DUFB incentive is place. I decided on using days as the unit of observation to increase the sample amount of data for estimating. I expect there to be enough transactions per day for this to be possible.

Let $y_{i}$ represent the outcome variable where $i=1,...,n$ denotes stores. Let $c$ denote the cutoff; $s_i$ the score computed for store $i$; and $D_i$ the assignment variable. Each draw (or row) of data for store $i$ is a vector $(y_i, s_i, D_i)$ corresponding to a single day. Recall that $y_i$ is a point statistics estimated using a single days worth of transaction data for store $i$. All days will be pooled, creating roughly $30\times5=150$ observations (days) per store.


Let $u_i$ be an error term assumed to be $\mathcal{N}(0, \sigma^2)$.

The RD model I propose is as follows

$$
y_{i} = \alpha + \rho D_i + \gamma (s_i - c) + \delta D_i(s_i - c) + u_{i}
$$

My hope is to produce a graph that looks like the following

[GENERATE EXAMPLE GRAPH. At each point $s_i$, there will be about 150 points drawn in vertical like. This would help visualize the distribution of average-dollars-per-day.]

### Main Analysis: Difference-in-Differences, Model 1 (DD1) {-}

[WRITE MODEL. Below is example of DD model overtime.]

```{r dd1-plot, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE, fig.height=4, fig.cap = "Example DD Model over 12 months in 2016 (DUFB start month 8)"}

DD1()

```

<!-- ### Main Analysis: Difference-in-Differences, Model 2 (DD2) -->

<!-- This model is some -->
<!-- The proposed model is as follows -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--   y_{ist}  &= \alpha_i + \beta_0 DUFB_{s} + \beta_1 POST_{t} + \delta (DUFB_{s} \cdot POST_{t}) + \sum_{j=1}^4 \theta_{j} I_{j}(t) + \epsilon_{ist} -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- where $y_{ist}$ is the outcome variable for store $i$ in state $s$ during week $t$. $\alpha_i$ captures any time-invariant store-specific effects. $DUFB_{s}$ indicates whether store $i$ has or will be part of the DUFB incentive. $POST_{t}$ indicates if week $t$ for store $i$ lands in a post- or pre-DUFB year. Recall, there are 3 years of data (2014 - 2016) and DUFB implementation is staggered across stores.  $I_{j}(t)$ captures any cyclical effects due to the monthly SNAP benefit transfer schedule. $I_{j}(t) = 1$ if week $t$ is the $j$th week of the month, where $j = 1,2,3,4$. -->


### Secondary Analysis: Difference-in-Differences with Matching {-}

Smaller analysis of the 3 stores that self-selected and were matched to 3 other stores. Total N will be 6. Likely that nothing will be significant. But need to be transparent.
