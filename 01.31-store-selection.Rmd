## Overview of Store Selection and Expansion {#store-selection-1 -}

How the 17 "treatment" stores and 15 "control" stores were selected in 2016 is important. First and foremost, selection was *not* random. Stores were either selected by the company (13 of 17) or self-selected into DUFB (4 of 17). Second, the 15 control stores were selected *after* the selection of the 17 treatment stores. Data from all remaining stores was requested but the request was denied; only 15 stores had been approved by the company's management. Finally, and most importantly, the selection criteria for the 17 treatment stores is *observable*. The implications of this will be covered in more detail in the [Methods](#methods) section.

### Selection and Expansion of DUFB Stores {-}

The first 2 stores were piloted with DUFB in 2014. Both were in geographically distinct areas (these will be referred to as "`Node 0`" and "`Node 1`"). There was a small expansion adding 3 more stores in 2015. The 3 stores were selected because they were geographically close to the 2 original pilot stores (2 close to `Node 0`, 1 close to `Node 1`). The 5 stores are referred to as the "core". The location of these 5 stores, separated in two clusters, established the geographic constraints that were then used to determine most of the additional stores in 2016.

DUFB was expanded to 12 more stores in 2016, totaling 17. Of those 12, 6 were selected due to their proximity to the 5 core stores, their SNAP EBT^[Electronic Benefit Transfer.] sales figures, and similarity in surrounding demographics (high population density, more African-American). In other words, 9 of the 17 stores---excluding the initial 2 pilot storesâ€”--were selected on a set of *observable* characteristics. The remaining 6 stores were not.

Of the remaining 6 stores, 4 asked if they could be included in the program. These stores *self-selected* into DUFB, making these stores fundamentally distinct. They were considered, and then included, only because they fell within the "Top 50". The final 2 stores were selected by the company for "strategic business decision". The best interpretation of this is that the company thought that DUFB would provide a competitive edge to the 2 included stores given some internal calculus. How the company came to this decision is *unknown* and therefore *unobserved*.

Table \@ref(tab:store-class) helps understand the year by year expansion of DUFB. Stores are classified as either `assigned`, `self-selected`, or `unobserved`. To be `assigned` means a stores participation in DUFB was determined (assigned) by the company; `self-selected` means the store asked the company to participate; `unobserved` means that the company selected the store to participate in DUFB but for unknown and unobserved reasons. Numbers were assigned to each store for easy reference but otherwise have no meaningful interpretation.


```{r store-class, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

tbl_df <- dufb_sn_df %>%
  dplyr::arrange(-dufb_2014, -dufb_2015, -dufb_2016, unknown) %>%
  dplyr::mutate(store_id = 1:length(store)) %>%
  dplyr::select(store, store_id, c(4:10)) %>%
  dplyr::mutate(type = if_else(self_selected == 1,
    'self-selected',
    'assigned')) %>%
  dplyr::mutate(type = if_else(unknown == 1, 'unobserved', type)) %>%
  dplyr::mutate(type = if_else(dufb_2014 == 1, 'pilot', type)) %>%
  dplyr::select(store_id, type, dplyr::contains('dufb'))

tbl <- tbl_df %>%
  dplyr::mutate(`2014` = if_else(dufb_2014 == 1, type, '')) %>%
  dplyr::mutate(`2015` = if_else(dufb_2015 == 1, type, '')) %>%
  dplyr::mutate(`2016` = if_else(dufb_2016 == 1, type, '')) %>%
  dplyr::select(store_id, `2014`:`2016`) %>%
  dplyr::rename(Store = store_id) %>%
  knitr::kable(., caption = 'Year by Year Store Selection. Stores 1 and 2 represent the initial 2014 pilot stores.', booktabs = TRUE)
tbl
```

### Expansion on Observables {-}

An example expansion on *observables* (using fake data) can be seen in Figure \@ref(fig:dufb-expansion). In the top frame, one can see two blue dots. These blue dots simulate the first two pilot stores in 2014. The left blue dot is `Node 0` and the right blue dot is `Node 1`. The gray zones represent areas of higher population density. Dark gray is considered *urban*, defined as having a population density of 1500 persons or more per square mile. The light gray are small towns and cities, more densely populated than very rural areas, but could not be considered *urban*. The expansion in 2015 (middle frame) proceeds to the stores closest to the original pilot stores. The expansion continues to 6 more stores in 2016 (bottom frame) away from the nodes but also along areas of higher population density.

Not conveyed in Figure \@ref(fig:dufb-expansion) is that the 2015 and 2016 expansions also move through stores that happen to be "highly ranked"---that is, have relatively higher SNAP EBT sales.^[All stores within the chain were ranked by SNAP EBT sales as a percentage of total sales.] Also not conveyed is the fact that there is a strong correlation between geography, population density, racial composition, and SNAP EBT sales. The 2015 expansion to the most nearby stores also meant that it was an expansion to stores with high SNAP EBT sales in densely populated, African-American neighborhoods. The 2016 DUFB expansion was more explicit given that set of feasible stores substantially increases as one moves away from each node. DUFB stores were thus specifically selected not just by geographic proximity, but also by SNAP EBT sales ranking and demographic compositions similar to the initial 2014 stores.

**Expansion Data**

Data for about each store was built by merging 4 different sources. The core data came from the grocery retailer directly, which provided a list of stores participating in DUFB from 2014 - 2016. The grocery retailer also provided a list of stores ranked by EBT sales as a fraction of total store sales and the size (square footage) of each store. Demographic and socioeconomic data came from the [Data Science Toolkit API](http://www.datasciencetoolkit.org/) (DSTK) and the [American Communities Survey API](http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html) (ACS). The DSTK API provides access to US Census data from 2000 at the *census block* level and the ACS API provides data spanning 2010 - 2014 at the *zip code* level. Lastly, data was extract by mining the [Family Fare](https://www.shopfamilyfare.com/store-locator) website.

Matching was done with the ACS data. The ACS zip code data was preferred because it provided income and housing data. Zip code level demographics are sufficiently descriptive; stores are evenly distributed across zip codes. Specifically, 58 stores are spread across 58 zip codes and 4 stores split between 2 zip codes (60 zip codes and 62 stores).^[I must also admit that my spatial and geocoding skills improved drastically in the months following the matching process. At the time, I did not know how determine census blocks from lat/long coordinates, relying on the DSTK API that did the conversion. The downside is that it returned 2010 data. I'm confident I could do it now, but I still think it's not worth the effort given the US Census Data excludes income data.]

Ideally, prior to matching, demographic data from the neighborhoods surrounding the store, who shopped at the store, and how the store was performing, its size, and goods made available would be known. Unfortunately, most of the publicly available data was not store-specific. The only store-specific data came either from the retail parent company directly or from mining the website.

```{r fig-dufb-expansion-setup, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

if(getOutputFormat() == "latex") pdf_yes = TRUE else pdf_yes = FALSE

# set up text
raw <- function(x) knitr::asis_output(x)

a <- list()

a[[1]] <- ("```{r dufb-expansion, fig.cap = 'Example expansion over time from 2014 to 2016 (top to bottom) using fake data. Blue dots denote stores with DUFB, pink dots denote without. Gray sectors denote higher population density. The initial nodes can be seen in the top (2014) frame.', fig.align='center', echo=FALSE}")
a[[2]] <- ("knitr::include_graphics('figures/expansion-v.pdf')")
if(!pdf_yes) a[[2]] <- gsub('pdf', 'png', a[[2]])
a[[3]] <- ("```")

out <- paste(a, collapse = '\n')
```

```{r fig-dufb-expansion-output, warning=FALSE, message=FALSE, eval=TRUE, echo=FALSE}
# MUST paste the output. cannot just knit!
raw(paste(knitr::knit(text = paste(out), quiet = TRUE)))
```

### Selection of Control Stores {-}

Ideally, all remaining stores would have been available to use as a control group but the company only approved that data be released for 15 stores. This left the added---and incredibly important---step of selecting the control stores since the company approved, but did not explicitly select, the 15 stores.

Selecting the control stores proceeded in two steps. First, stores that either self-selected or were selected using some unobservable criteria were matched using *Coarsened Exact Matching* (CEM) [@iacus_causal_2011]. Second, stores assigned DUFB were pooled with nearby control stores and then scored using a linear probability model. Each step is explained in detail.


*Step 1: Coarsened Exact Matching*

The 6 stores classified as `self-selected` or `unobserved` (stores `12` through `17`; see Table \@ref(tab:store-class)) were compared against all possible control stores for matches. Matching was done across 5 dimensions: race, income, population density, store attributes, store EBT sales. One variable per dimension was selected: percentage of population that is African-American (zip code level); people per square mile (zip code level); median income for people who have received SNAP or similar assistance (zip code level); the number of associates employed in each store; and the percentage of total stores sales attributed to EBT/SNAP.

Of the 6 stores (stores `12` - `17`), only 3 produced viable matches. However, each of the 3 matched stores had matched to more than one control stores. The closest stores, by driving distance, were selected as the tie-breaker for each matched store. Stores were sufficiently far apart, with very sparsely populated areas between, that "spill-over" was considered unlikely. That is, it is considered unlikely that a shopper near a store without DUFB would opt to drive 30 or more minutes to shop at the store *with* DUFB.

This left 12 stores to be allotted to the control group and 3 treatment stores to be effectively discarded.


*Step 2: Scoring via Linear Probability Model*

Assignment to treatment and control can be perfectly determined since we know and observe the criteria used for assignment: geographic distance from an initial store (node), SNAP EBT sales rank, and demographics---specifically population density and percentage African-American.^[It should be noted that the company did not explicitly say population and race were part of the selection criteria. Instead, they said something along the lines of "stores serving a similar population as the original stores."] A scoring function was created by fitting a linear probability model to all stores within 140 kilometers of the two initial pilot stores.

$$
\begin{aligned}
  \bm{s}  &= \widehat{P(\mathbf{D} = 1 | \bm{X}, \bm{N})} \\
          &= \mathbf{X} \bm{\hat \beta} + \hat \alpha \mathbf{N} + \left (\mathbf{X} \odot \mathbf{N} \right ) \bm{\hat \gamma}
\end{aligned}
$$

$\bm{s}$ are the fitted values of the estimated linear probability model; $\mathbf{D} \in \{0,1 \}$ is a $n \times 1$ vector of store assignments to DUFB; $\mathbf{X}$ is an $n \times k$ matrix of normalized observable covariates that determine assignment; $\mathbf{N} \in \{0, 1 \}$ is an $n \times 1$ dummy vector denoting the closest pilot store aka "Node", where $0$ is `Node 0` and $1$ is `Node 1`. $\odot$ represents element-wise multiplication aka "Hadamard product".

Stores were sorted by the fitted values of the model, $\bm{s}$. There is perfect separation between DUFB stores and those without (see Figure \@ref(fig:score-plot)). Therefore, the top 11 stores by score value are all DUFB stores. The next 12 stores by score value are then allotted to the control group.

```{r score-plot, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE, fig.height=4, fig.cap = "Store Score vs DUFB Assignment"}
# d <- score_controls(metric='drive')
# saveRDS(d, 'data/score-metric-driving.rds')
d <- readRDS('data/score-metric-driving.rds')
dd <- d %>%
  dplyr::slice(1:23) %>%
  dplyr::mutate(assignment = 1 - control) %>%
  dplyr::select(score, assignment) %>%
  dplyr::mutate(`DUFB` = dplyr::if_else(assignment == 1, 'Yes', 'No'))

g1 <- ggplot2::ggplot(data = dd) +
    ggplot2::geom_point(aes(x = score, y = assignment, colour = `DUFB`), size = 3) +
    theme_clean()

g1 + geom_vline(aes(xintercept = .57), color = 'blue', linetype = 'dashed')
```

### Motivation for Matching {-}

Not all treated stores will be matched to a control. As mentioned, this is due to the nature of how the 17 treated stores were selected. The parent company intentionally selected stores with some of the highest EBT (aka SNAP Electronic Benefit Transfer (EBT) Card) sales that were also within relatively similar geographic locations. This reduced the burden of advertising and implementing DUFB for The parent company. The unfortunate downside of this implementation is that it effectively removed any likely matches for treated stores located in the most Urban areas (e.g. Grand Rapids and Battle Creek).

Here is an example to illustrate why it is infeasible to matching all treated stores and instead expand selection algorithmically on observables. If we calculate the percentage of the population by *zip code* that is African American then split the data into treatment and control groups, we get the following:

```{r ex-pct-black, echo=FALSE, eval=TRUE, message=FALSE}
load('data/env0')

dt1 <- dufb_ff_sn_df %>%
  dplyr::left_join(mi_acs %>% dplyr::select(-lat, -long), by = 'postal_code') %>%
  dplyr::mutate(pct_black = 100*pop_black/pop_total) %>%
  dplyr::mutate(treated = dufb_2016) %>%
  dplyr::select(-starts_with('dufb'), -ebt_rank) %>%
  dplyr::filter(!is.na(pct_black)) %>%
  data.frame()

x <- mean(dt1$pct_black[dt1$treated == 1]) - mean(dt1$pct_black[dt1$treated == 0])
sprintf("Difference in Means (Treated - Control) = %f\n", x) %>% cat

cat("\nPopulation, % Black (Treated, Top 10):\n")
sort(dt1$pct_black[dt1$treated == 1], decreasing = TRUE) %>% head(10)

cat("\nPopulation, % Black (Control, Top 10):\n")
sort(dt1$pct_black[dt1$treated == 0], decreasing = TRUE) %>% head(10)
```

What these results tell us is how potentially distinct the populations are within the zip codes containing the treated stores. Sorting population percentages in descending order, no good match exists within the control stores for the top 7 treated stores. One variable is the simplest case; matching only gets more difficult as one brings in more variables to match.

Considering the separation between some of the treated stores and all of the control stores, it was prudent to rethink the store selection and matching strategy.

It must be noted that matching is not a necessary step during every design phase. It is, in large part, a way to hedge against the possibility that merely selecting the next top 15 stores by EBT sales could sour the estimates. Matching a smaller set of treatment stores against a larger pool of controls can often produce estimates less sensitive to even the smallest changes in some model specifications [@imbens_causal_2015]. However, other models and tools (like regression) are in relatively unperturbed by a lack of design-phase matching, but still benefit from having a larger sample size [@angrist_mostly_2008].

### Matching Details {-}

Like most data-dependent endeavors, the most tedious part of matching the stores was obtaining enough variables. Once enough data were obtained, variables were selected on how best they captured data from the following dimensions:

- Demographics (e.g. race)
- Income/wealth
- Population density (e.g. urban vs rural)
- Store attributes
- Store EBT sales

One may assume that more variables makes matching easier. This is only true insofar as it provides one with a large pool of options. It is still necessary to carefully select how many variables one is using because matching becomes more and more difficult with each added variable used. This is especially true with a small sample size.

The matching covariates that were finally selected are:

- `pct_black` : Percentage of population that is black (zip code level)
- `dens_pop` : The population density (people per square mile, zip code level)
- `income_p50_snap_yes` : Median income for people who have received SNAP or similar assistance (zip code level)
- `store_n_associates` : The number of associates employed in each store.
- `ebt_sales_pct` : Percentage of total stores sales attributed to EBT/SNAP.

&nbsp;

**Results of Match**

```{r match-results, echo = FALSE, eval=TRUE}
env0$cem_mat$tab %>% knitr::kable(., caption = "CEM Match Matrix", booktabs = TRUE)
```

`G0` represents the "control" group and `G1` represents the "treated" group. One can observe that `r env0$cem_mat$tab[2,2]` "treated" stores were matched to `r env0$cem_mat$tab[2,1]` "control" stores. Each of the `r env0$cem_mat$tab[2,2]` treated stores was matched to its closest control store by driving distance.


**Covariate Cut-points**

The CEM procedure depends heavily on the "cut-points" selected for each variable. This is akin to setting the cut-off points when turning a continuous variable into a categorical variable. For example, when converting income values from dollars into `low-` `middle-` and `high-income` groups, at least 4 cut-points are required (2 of which are the maximum and minimum). What the other 2 cut-points are will greatly affect the match. This leads to the question, for example, should the cut-points be `25000` and `100000` or perhaps the median and the top 10%?

For the matches produced, the following cut-points were created.

```{r, echo = FALSE, eval = TRUE}
env0$cem_mat$breaks
```

Understanding why is best explained using a visualization. Below are graphs of the variables `pct_black` and `income_p50_snap_yes` with their corresponding cut-points. The aim of each cut-point is to balance the creation of reasonably sized partitions while still marking obvious shifts in the underlying distribution.

For example, in the first plot (`pct_black`), there are clearly points where the slope dramatically increases --- and then spikes --- in the percentage of African Americans. But in the second plot, the slope is more gradual, so the partitioning is aimed more at getting relatively balanced groups.

```{r, echo = FALSE, eval = TRUE, screenshot.opts = list(delay = 3, cliprect = 'viewport')}
env0$plt1
```

```{r, echo = FALSE, eval = TRUE, screenshot.opts = list(delay = 3, cliprect = 'viewport')}
env0$plt2
```
