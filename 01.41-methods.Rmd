### Difference-in-Difference-in-Differences (DDD) {-}

I observe transaction $i=1,...,L$ in store $j=1,...,N$ across $t=1,...,T$ days. Let $y_{ijt}$ be total FV expenditures for transaction $i$ in store $j$ on day $t$. The DDD regression is

$$
\begin{aligned}
y_{ijt} &= \alpha_0 + \alpha_1 dE_j + \alpha_2 dS_i  + \alpha_3 dE_j \cdot dS_i \\& \quad + \theta_1 dP_t + \theta_2 dP_t \cdot dE_j + \theta_3 dP_t \cdot dS_i \\
& \quad + \delta dE_j \cdot dS_i \cdot dP_t + \bm{x'}_{ijt} \bm{\beta} + \lambda_t + \epsilon_{ijt}
\end{aligned}
$$

where $dE_j$ represents store assignment to DUFB group, $dS_i$ represents a SNAP or SNAP related transaction (target group), and $dP_t$ represent the treatment period, August - December. $\lambda_t$ captures daily (time) effects and $\bm{x'}_{ijt}$ is a vector of observable characteristics about transaction $i$ in store $j$ on day $t$. The coefficient of interest is $\delta$. $\epsilon_{ijt}$ are idiosyncratic errors at the transaction level.

Again, I do not observe individuals, only transactions. Yet I think it reasonable to assume that the structure of daily transaction data more closely resembles that of repeated cross-sections than of panel data. Assuming it was possible to link individuals to purchases and build panel data, the same individual would likely be observed *sporadically* within in a given month. That is, were this to be panel of the same $N$ individual shoppers across $T$ total days, many---if not most---of those days would have missing data. There would certainly be shoppers observed multiple times per week, but I expect such shoppers to be rare. I certainly would not expect to have a balanced panel and no shopper would be observed all 365 days.

I therefore find it reasonable to treat each day of observed data as a single independent cross-section of $L$ transactions generated by $N \le L$ unknown individuals across $J$ many stores. Aligned sequentially, these form repeated cross-sections over-time. Each transaction also falls naturally into a cluster---the store where it occurred---that is time-invariant and determined prior to the data being collected.

The model DDD model above can be improved by introducing store effects. These effectively capture DUFB assignment and all other time-invariant store-level characteristics. The $dE$ dummy, for example, drops out. The notation can also be cleaned up by condensing the other dummies, emphasizing only variation. The spruced up model is

\begin{equation}
y_{ijt} = \gamma_j + \lambda_t + \phi_0 D_i + \phi_1 D_{ij} + \phi_2 D_{it}+ \phi_3 D_{jt} + \delta_t D_{ijt} + \bm{x'}_{ijt} \bm{\beta} + \epsilon_{ijt}
(\#eq:ddd)
\end{equation}

where $\gamma_j$ now represents store effects, $dS_i \equiv D_i$, and the remaining dummy variables $D_{ij},~D_{jt},~D_{it},~D_{ijt}$ represent the dimensions along which they vary---$i$ transaction type (SNAP or not), $j$ store experiment group, and $t$ day during DUFB treatment period (August - December). To capture more detail than just the average, $\delta_t$ is allowed to vary by day.

**Unobserved Effects**

I still do not know what the transaction characteristic variables will be and hence do not know what variables go into $\bm{x'}_{ijt}$. I do know, however, that without panel data, I have no methods for dealing with unobserved individual effects. That is, some unobserved individual effect $c_i$ likely exists such that $e_{ijt} = c_i + u_{ijt}$ where $\exists t \ni E[\bm{x'}_{ijt}c_i] \ne 0$. In short, my estimates will be biased due to an omitted variables problem.

I'm am still thinking about how I can capture part of the unobserved individual effect $c_i$. I am open to suggestions.

---

### Tobit and other Hurdle Models {-}

Calculating fruit and vegetable expenditures, $y_{ijt}$, for each transaction will result with a non-trivial amount of zeros. These zeros are not "censored" values in the Heckman selection problem sense. They are genuine zeros aka "corner solution". But the mechanisms behind the zeros is unknown.

**Tobit Model**

The Tobit model is agnostic to the economic mechanism generating corner solutions. It is the basic approach when little else is known other than the decision not to purchase fruits and vegetables is resulting in zero-expenditures. The basic structure is

$$
\begin{aligned}
y^* &= x'\beta + u \\
y &= max(0, y^*)
\end{aligned}
$$

where $y^*$ is a latent variable and $y$ is observed. Tobit can be generalized beyond requiring homoskedastic error terms but requires normality. This is important because error terms in repeated cross sectional data are assumed independent *not* identically distributed. In other words, heteroskedastic. Other than error term specifications, any additive and linear-in-parameters regression can be estimated using Tobit. In other words, I can set $y^*$ equal to equation \@ref(eq:ddd).

For details on the likelihood functions, see @amemiya_tobit_1984.

**Hurdle and (Naive) Two-Part Models**

Hurdle models generally have two parts (i.e. "Double" Hurdle)---a decision-to-buy (participate) and the amount to purchase (consumption) [@jones_double-hurdle_1989]. Let $I^* \in {0,1}$ indicate the decision to purchase fruits and vegetables and $y^*$ be the amount of dollars spent. Both are latent variables. Let $y$ be observed expenditures. Formally,

$$
\begin{aligned}
I^* &= z'\gamma + v \\
y^* &= x'\beta + u  \\
y   &= I^* \times max(0, y^*) \\
& \phantom{x}\\
(u,v) & \sim N(0,\Omega) \\
\Omega & = {\left (
\begin{array}{cc}
  \sigma^2_{u} & \sigma_{uv} \\
  \sigma_{uv} & \sigma^2_{v}
\end{array}
\right )}
\end{aligned}
(\#eq:dhurdle)
$$

The @jones_double-hurdle_1989 "Full" Double Hurdle model and the @cragg_statistical_1971 have the same framework. The distinction is that in the Cragg model assumes no correlation between $u$ and $v$ (i.e. $\sigma_{uv} = 0$). The likelihood function for the Cragg model is, in other words, a simplification of the Jones model. Formally, the Jones model is

$$
\begin{aligned}
L &= \prod_0
  \left [
    1 - \Phi
    \left (
      \frac{z'\gamma}{\sigma_v},
      \frac{x'\beta}{\sigma_u},
      \rho
    \right )
  \right ] \times \\
& \quad ~ \prod_{+} \Phi
  \left [
      \frac{ \left (
        \frac{z'\gamma}{\sigma_v} +
        \rho(y^* - x'\beta) \right )}
         {\left ( \sqrt{1 - \rho^2} \right ) } \right ]
\frac{1}{\sigma_u} \phi \left ( \frac{y^* - x'\beta}{\sigma_u} \right )
\end{aligned}
(\#eq:lhurdle)
$$

Once again, setting the DDD to be $y^*$ is not a problem. The main problem is determining the participation equation $I^*$. What factors variables participation i.e. the decision-to-buy fruits and vegetables? Variables about individual characteristics would certainly be helpful here but that isn't possible. Some mechanism is driving consumers to buy or not buy fruits and vegetables. The likeliest candidate is data on *other* products purchased within a given transaction. The existence of complimentary goods (e.g. olive oil, meats, etc.) may increase the likelihood of observing FV purchases within the same trip. The size of the shopping basket likely increases overall chances of FV purchases. Day of the week or week of the month effects will also matter, since government benefits and pay schedules may increase the chances of shopping trips in aggregate.

I anticipate estimating both the Jones and Cragg model as a robustness check. But I do not expect $Cov(\sigma_u, \sigma_v) = 0$ given I cannot capture individual effects. Unobserved individual effects affect both participation and consumption. In both equations, the error term absorbs the individual effect, making them correlated by construction.

I will also estimate a "Naive" Two-Part model, where the participation and consumptions equations are estimate independently from the other (Probit + OLS). Again, just another point of comparison.

**Multiple Discrete-Continuous Extreme Value Models**

The @bhat_multiple_2005 Multiple Discrete-Continuous Extreme Value (MDCEV) framework allows for choice along a vector of non-mutually exclusive goods. Non-negative consumption is allowed across all goods.

I will attempt to use a later iteration of the MDCEV model from @bhat_multiple_2008. The distinction that interest me is that the Bhat (2008) model allows for price variation across goods and explicitly formulates the Kuhn-Tucker constraints using expenditures. The econometric model is as follows.

Utility from purchasing vector of goods $\bm(x) = (x_1, x_2,...,x_k)$ is defined as

$$
\begin{aligned}
\tilde{U} &= \sum_k \frac{\gamma_k}{\alpha^*_k} \psi(z_k, \epsilon_k)
  \left [
    \left (
      \frac{y_k}{\gamma_k p_k} + 1
    \right )^{\alpha_k} - 1
  \right ] \\
\psi(z_k, \epsilon_k) &= exp(z'_k\beta + \epsilon_k)
\\
\sum_k p_k &= Y
\end{aligned}
$$

where $z_k$ is a vector of attribute variables about product $k$ and of the consumer, $y_k$ is expenditure on product/good $k$, $p_k$ is the price, and $Y$ is total expenditure on basket of goods $\bm(x)$. $\epsilon_k$ are idiosyncratic shocks with an extreme-value distribution. To understand the role of $\psi_k$, $\alpha_k$ and $\gamma_k$, see @bhat_multiple_2008 for details.

Using the first good as a reference group, the KT conditions that solve the optimal expenditure problem (the Lagrangian above) are

$$
\begin{aligned}
V_k^* + \sigma \epsilon_k &= V_1^* + \sigma \epsilon_1 \text{ if }
  y_k^* > 0 (k = 2,3,...,K) \\
V_k^* + \sigma \epsilon_k &< V_1^* + \sigma \epsilon_1 \text{ if }
  y_k^* = 0 (k = 2,3,...,K), \text{ where } \\
V_k^* &= \sigma z'_k \beta + \sigma (\alpha_k - 1)
  \text{ln} \left ( \frac{y_k}{\gamma_k p_k} + 1 \right ) - \text{ln} p_k
\end{aligned}
$$

where $V_k$ is identified only when $\alpha_k$ or $\gamma_k$ is fixed (both terms estimate related "satiation" behaviors). See @bhat_multiple_2008 (equations 18 and 19) for the Jacobian and closed form expression for the probability of spending $y_k^*$. Example likelihood function to solve the equation for $i=1,...,L$ transaction (or $N$ individuals) in a given cross-section can be found in @bhat_multiple_2005 (equation 18) and @bhat_household_2006 (equation 8).

**Expected Challenges with this Model**

Despite the theoretical advantages of the MDCEV framework (i.e. optimization over multiple goods allowing corner solutions), there are a few challenges I anticipate with using the MDCEV framework.

The first is a concern about prices. The most effective way to incorporate price variation is to make the basket of available goods equal to the full universe of observed products. This would likely be huge. The MDCEV framework is flexible enough to do it, but my fear is that it will lead to some difficulties in interpretation. In reality, however, I don't care as much about expenditure at the product level as much as I do about expenditure on particular types. That is, I care more about spending on FVs versus non-FVs. Therefore, at the simplest level, my vector of possible goods would be just 2. However, how would I price FV versus non-FV? I could construct a price index for just those two groups but it would combine far too many distinct food types to be reasonable. Moving towards something like having between 20 to 40 general food categories seems like better approach. For example, @harding_effect_2014 estimate the prices for 33 different product groups by using the Stone price index, which depends only on observable price values.

The second concern is programming related. There are no available packages that implement the MDCEV package. I would have to adapt the GAUSS code provided by Bhat on his website in order to get the model running. This isn't a concern about being able to do it as much as the amount of time it would require to learn/understand GAUSS in order to implement it in R or Python.^[It looks like some folks at the company [Mobility Analytics](http://www.mobilityanalytics.org) have started, which is very promising.]

---
